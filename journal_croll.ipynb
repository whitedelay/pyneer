{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##크롤링 대상 사이트 : https://www.computer.org (IEEE)\n",
    "\n",
    "#사용할 라이브러리\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from multiprocessing import Pool, Array, Process\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url 크롤링하는 함수 정의\n",
    "\n",
    "def croll_link(next_url,link_name):\n",
    "    req = requests.get('https://www.computer.org'+next_url)\n",
    "    soup = bs(req.text, 'html.parser')\n",
    "    next_r_url = soup.select(link_name)\n",
    "    \n",
    "    parsed = [] \n",
    "    for i in range(0,len(next_r_url)):\n",
    "        parsed.append(next_r_url[i].get(\"href\")) #url 추출해서 list에 append\n",
    "    \n",
    "    return parsed #list 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.603402137756348\n"
     ]
    }
   ],
   "source": [
    "#크롤링 - site에서는 주제-연도-달(2-3달)-내용 으로 되어있음\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# computer science subject link croll\n",
    "transactionLinks = croll_link('/csdl','a.transactionLink')\n",
    "\n",
    "# subject -> volume(year)\n",
    "volumeLinks = croll_link(transactionLinks[0],'a.volumeLink')\n",
    "\n",
    "# volume -> issue(months)\n",
    "issueLinks = croll_link(volumeLinks[0],'a.issueLink')\n",
    "\n",
    "# issue -> contents\n",
    "abstractLinks = croll_link(issueLinks[0],'div.col-xs-12 > div > div > div > button > a') # contents\n",
    "\n",
    "# abstractLinks에서 html만 추출 (javascript와 pdf도 태그가 같았기에 soup로 구분하지 못했음)\n",
    "new_absLinks = []\n",
    "for i in range(0,len(abstractLinks)):\n",
    "    if(abstractLinks[i][-8:] == 'abs.html'):\n",
    "        new_absLinks.append(abstractLinks[i])\n",
    "        \n",
    "close = time.time()\n",
    "\n",
    "print(close-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstractLinks = new_absLinks\n",
    "new_absLinks = []\n",
    "\n",
    "#abstractLinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contents\n",
    "req = requests.get('https://www.computer.org'+abstractLinks[1])\n",
    "soup = bs(req.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simulating the Large-Scale Erosion of Genomic Privacy Over Time'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#title 크롤링\n",
    "abs_title = (soup.select('div.abstractTitle > a > h2')[0].text) # title 추출\n",
    "abs_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018/05'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url에 date내용이 들어있음\n",
    "abs_yymm = abstractLinks[1][15:22]\n",
    "abs_yymm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contents 크롤링\n",
    "abs_contents = soup.select('div.abstractText.abstractTextMB')[0].text\n",
    "#abs_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data set 늘리기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month를 다 합쳐봄 - volume은 같음\n",
    "# 다 하면 너무 많을 것 같아서 일단 issue(month)만 합쳤음\n",
    "\n",
    "total_absLink = []\n",
    "for i in range(0,len(issueLinks)):\n",
    "    total_absLink += croll_link(issueLinks[i],'div.col-xs-12 > div > div > div > button > a')\n",
    "\n",
    "#total_absLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_absLink = []\n",
    "for i in range(0,len(total_absLink)):\n",
    "    if(total_absLink[i][-8:] == 'abs.html'):\n",
    "        final_absLink.append(total_absLink[i])\n",
    "#final_absLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.53028893470764\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = str(datetime.now())[0:19]\n",
    "start = time.time()\n",
    "def croll_abs(link):\n",
    "    req = requests.get('https://www.computer.org'+link)\n",
    "    soup = bs(req.text, 'html.parser')\n",
    "    journal = {}\n",
    "    journal['title'] = soup.select('div.abstractTitle > a > h2')[0].text\n",
    "    journal['abstract'] = soup.select('div.abstractText.abstractTextMB')[0].text\n",
    "    journal['date'] = link[15:22]\n",
    "  \n",
    "    filename = (\"./\"+\"%s\"+\"_output.txt\") % now\n",
    "    with open(filename, 'a') as f:\n",
    "    #for data in journal_list:\n",
    "        o_data = \"%s\\t%s\\t%s\\n\" % (journal['date'], journal['title'],journal['abstract'])\n",
    "        f.write(o_data)\n",
    "\n",
    "pool = Pool(processes=4)\n",
    "p = pool.map(croll_abs,final_absLink)\n",
    "\n",
    "close = time.time()\n",
    "print(close-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
