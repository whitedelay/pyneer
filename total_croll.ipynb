{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##크롤링 대상 사이트 : https://www.computer.org (IEEE)\n",
    "## https://ieeexplore.ieee.org 여기와 같은 자료\n",
    "\n",
    "#사용할 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests # 크롤링\n",
    "from bs4 import BeautifulSoup as bs # 크롤링\n",
    "from multiprocessing import Pool # 멀티 프로세싱\n",
    "from functools import partial # pool.map에서 여러 매개 변수를 받기 위함\n",
    "import time # 시간 기록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url 크롤링하는 함수 정의\n",
    "\n",
    "#하나의 url이 주어지면 해당 태그의 url전부 추출하여 list로 반환\n",
    "def get_link(url,tag_name):\n",
    "    req = requests.get('https://www.computer.org'+url)\n",
    "    soup = bs(req.text, 'html.parser')\n",
    "    tag_list = soup.select(tag_name)       \n",
    "\n",
    "    link = [] \n",
    "    for i in range(0,len(tag_list)):\n",
    "        link.append(tag_list[i].get(\"href\")) # url이 있는 href만 추출\n",
    "        \n",
    "    return link #list 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# computer science topic url list\n",
    "topic_href = get_link('/csdl','a.transactionLink')\n",
    "\n",
    "\n",
    "abstract_html = [] #abstract url을 저장할 리스트\n",
    "\n",
    "pool = Pool(processes=32) # 프로세스 32개\n",
    "\n",
    "#멀티 프로세싱을 이용하여 href를 추출하고, pool.map의 반환 값이 2차원 배열이기 때문에 1차원으로 합치는 sum함수 사용\n",
    "#pool.map(function,list) : 해당 function에 list의 값들을 매핑\n",
    "\n",
    "#topic url -> year url\n",
    "#get_link의 tag_name 매개변수 값은 'a.volumeLink'\n",
    "year_href = sum(pool.map(partial(get_link, tag_name='a.volumeLink'),topic_href),[]) # 해당 태그를 인자로 주고, 주어진 topic url들 속의 href들을 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "398"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(year_href) # 모든 topic들의 년도 url개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.80996823310852\n"
     ]
    }
   ],
   "source": [
    "#year url -> months url\n",
    "start = time.time()\n",
    "months_href = sum(pool.map(partial(get_link, tag_name='a.issueLink'),year_href),[])\n",
    "close = time.time()\n",
    "\n",
    "print(str(close-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3211"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(months_href) # 모든 months들의 url 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.29295206069946\n"
     ]
    }
   ],
   "source": [
    "#months url -> abstract url\n",
    "start = time.time()\n",
    "abstract_href = sum(pool.map(partial(get_link, tag_name='div.col-xs-12 > div > div > div > button > a'),months_href),[])\n",
    "abstract_html.extend([ x for x in abstract_href if x[-8:] == \"abs.html\"]) # pdf와 javascript 제거\n",
    "close = time.time()\n",
    "print(str(close-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49653"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abstract_html) # 총 abstract url 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '\\n'로 구분하여 전체 abstract url주소들을 파일로 저장\n",
    "with open(\"./output/all_html.txt\", 'w') as f: \n",
    "    for x in abstract_html:\n",
    "        f.write(\"%s\\n\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abstract url에서 topic,title,issn,abstract,date값 추출하는 함수\n",
    "def croll_abs(link):\n",
    "    req = requests.get('https://www.computer.org'+link)\n",
    "    soup = bs(req.text, 'html.parser')\n",
    "    journal = {}\n",
    "    journal['topic'] = soup.select('a.breadCrumbLink')[1].get('title')\n",
    "    journal['title'] = soup.select('div.abstractTitle > a > h2')[0].text\n",
    "    journal['ISSN'] = (soup.select('div.abstractText')[1].text)[-9:]\n",
    "    journal['abstract'] = soup.select('div.abstractText.abstractTextMB')[0].text\n",
    "    journal['date'] = link[15:22]\n",
    "    \n",
    "    return journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes=64)\n",
    "start = time.time()    \n",
    "result = pool.map(croll_abs,abstract_html)\n",
    "close = time.time()\n",
    "print(str(close-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
